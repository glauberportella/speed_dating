{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dating = pd.read_csv('dating-full.csv')\n",
    "count_quotes = 0\n",
    "count_lowercase = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_quotes(column):\n",
    "    global count_quotes\n",
    "    count = column.str.count(\"\\'.*\\'\").sum()\n",
    "#     print (count)\n",
    "    if count_quotes != count:\n",
    "        count_quotes += count\n",
    "    return column.str.strip('\\'')\n",
    "\n",
    "def convert_lowercase(column):\n",
    "    global count_lowercase\n",
    "    count_lowercase = len(column)- column.str.islower().sum()\n",
    "#     print (count_lowercase)\n",
    "    return column.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dating[['race','race_o','field']] = dating[['race','race_o','field']].apply(remove_quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dating[['field']] = dating[['field']].apply(convert_lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Quotes removed from', count_quotes, 'cells')\n",
    "print('Standardized', count_lowercase , 'cells to lower case.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_encoder_by_field = {}\n",
    "def get_encoding(column):\n",
    "    column = column.astype('category')\n",
    "    encoding = {}\n",
    "    for i, category in enumerate(column.cat.categories):\n",
    "        encoding[category] = i\n",
    "    global_encoder_by_field[column.name] = encoding\n",
    "    return column.cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dating[['race','race_o','gender','field']] = dating[['race','race_o','gender','field']].apply(get_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Value assigned for male in column gender:', global_encoder_by_field['gender']['male'])\n",
    "print ('Value assigned for European/Caucasian-American in column race:', global_encoder_by_field['race']['European/Caucasian-American'])\n",
    "print ('Value assigned for Latino/Hispanic American in column race o:', global_encoder_by_field['race_o']['Latino/Hispanic American'])\n",
    "print ('Value assigned for law in column field:', global_encoder_by_field['field']['law'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "partner_cols = ['pref_o_attractive','pref_o_sincere','pref_o_intelligence','pref_o_funny','pref_o_ambitious','pref_o_shared_interests']\n",
    "participant_cols = ['attractive_important', 'sincere_important', 'intelligence_important', 'funny_important', 'ambition_important', 'shared_interests_important']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_partner = 0\n",
    "total_participant = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range (0,6):\n",
    "    total_partner += dating[partner_cols[i]]\n",
    "    total_participant += dating[participant_cols[i]]\n",
    "# print (total_partner)\n",
    "# print (total_participant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,6):\n",
    "    dating[partner_cols[i]]/=total_partner\n",
    "    dating[participant_cols[i]]/=total_participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,6):\n",
    "    participant_mean = dating[participant_cols[i]].sum()/len(dating[participant_cols[i]])\n",
    "    print ('Mean of ', participant_cols[i], ':', round(participant_mean, 2))\n",
    "for i in range(0,6): \n",
    "    partner_mean = dating[partner_cols[i]].sum()/len(dating[partner_cols[i]])\n",
    "    print ('Mean of ', partner_cols[i], ':', round(partner_mean, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dating.to_csv('dating.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_print_participant_mean(dataframe):\n",
    "    mean_scores = []\n",
    "    for i in range(0,6):\n",
    "        participant_mean = dataframe[participant_cols[i]].sum()/len(dataframe[participant_cols[i]])\n",
    "        print ('Mean of ', participant_cols[i], ':', round(participant_mean, 2))\n",
    "        mean_scores.append(participant_mean)\n",
    "    return mean_scores\n",
    "\n",
    "dating_female = dating[dating['gender'] == 0]\n",
    "dating_male = dating[dating['gender'] == 1]\n",
    "female_mean_scores = get_print_participant_mean(dating_female)\n",
    "male_mean_scores = get_print_participant_mean(dating_male)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "ind = np.arange(6)\n",
    "width = 0.35\n",
    "p1 = plt.bar(ind, female_mean_scores, width,color = 'pink')\n",
    "p2 = plt.bar(ind+width, male_mean_scores, width, color = 'blue')\n",
    "plt.ylabel('Mean Scores')\n",
    "plt.title('Preference scores of participants by gender')\n",
    "plt.xticks(ind+width/2, (participant_cols[0], participant_cols[1], participant_cols[2], \n",
    "                 participant_cols[3], participant_cols[4], participant_cols[5]), rotation=80)\n",
    "plt.yticks(np.arange(0,0.5,0.05))\n",
    "plt.legend((p1[0], p2[0]), ('Female', 'Male'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distinct_values_rating_partner(dataframe, attribute):\n",
    "    print (dataframe[attribute].nunique()) \n",
    "    return (dataframe[attribute].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_partner_participant = ['attractive_partner','sincere_partner','intelligence_parter', \n",
    "                             'funny_partner', 'ambition_partner', 'shared_interests_partner']\n",
    "unique_values = []\n",
    "for i in range(0,6):\n",
    "    unique_values.append(get_distinct_values_rating_partner(dating, rating_partner_participant[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_success_rate(dataframe, attribute, value):\n",
    "    dating_attribute_value = dataframe[dataframe[attribute] == value]\n",
    "    dating_success = dating_attribute_value[dating_attribute_value['decision'] == 1]\n",
    "    return len(dating_success)*1.0/len(dating_attribute_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_success_rate(dating, rating_partner_participant[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "success_rates_all_attributes = []\n",
    "for i in range(6):\n",
    "        success_rate_attribute = []\n",
    "        for value in unique_values[i]:\n",
    "            success_rate_attribute.append(get_success_rate(dating, rating_partner_participant[i], value))\n",
    "        success_rates_all_attributes.append(success_rate_attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (success_rates_all_attributes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "for i in range(6):\n",
    "    area = np.pi*3\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.scatter(unique_values[i], success_rates_all_attributes[i], s=area)\n",
    "\n",
    "    plt.title('Scatter Plot for Partners who Perform Well on ' + rating_partner_participant[i])\n",
    "    plt.xlabel('Attribute Value for ' + rating_partner_participant[i])\n",
    "    plt.ylabel('Success Rate')\n",
    "\n",
    "    plt.xticks(np.arange(0,11,1))\n",
    "    plt.yticks(np.arange(0,1.1,0.1))\n",
    "    plt.show()\n",
    "    plt.savefig('scatter_plot_' + rating_partner_participant[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dating = pd.read_csv(\"dating.csv\")\n",
    "'''\n",
    "clean data for columns gaming and reading\n",
    "'''\n",
    "column = 'gaming'\n",
    "range_highest = 10\n",
    "dating.loc[dating[column] > range_highest,column] = range_highest\n",
    "dating.loc[dating['reading'] > range_highest,'reading'] = range_highest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_binned_column(dataframe, column, num_bins, bin_range):\n",
    "    dataframe[column] = pd.cut(dataframe[column], bin_range, include_lowest = True,\n",
    "                               labels = np.arange(num_bins), retbins = False)\n",
    "    return dataframe[column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_binned_cols = ['gender', 'race', 'race_o', 'samerace', 'field', 'decision']\n",
    "age_cols = ['age', 'age_o']\n",
    "num_bins = 5\n",
    "for column in dating:\n",
    "    if column not in non_binned_cols:\n",
    "        bin_range = np.arange(0,11,(10-0)/num_bins)\n",
    "        '''\n",
    "        Change bin range if needed\n",
    "        '''\n",
    "        if column in age_cols:\n",
    "            bin_range = np.arange(18,59,(58-18)/num_bins)\n",
    "#             print (\"bin range for age \", column)\n",
    "        elif column in partner_cols or column in participant_cols:\n",
    "            bin_range = np.arange(0,1.1,(1.0)/num_bins)\n",
    "#             print (\"different bin range \", column)\n",
    "        elif column == 'interests_correlate':\n",
    "            bin_range = np.arange(-1,1.1,(1+1)/num_bins)\n",
    "#             print (\"interest column\", column)\n",
    "        \n",
    "        '''\n",
    "        get binned column\n",
    "        '''\n",
    "        dating[column] = get_binned_column(dating, column, num_bins, bin_range)\n",
    "        print (column, \": \", dating[column].values_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in dating:\n",
    "    if column not in non_binned_cols:\n",
    "        count = dating[column].value_counts(sort=False)\n",
    "        print (column, \": \", count.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dating.to_csv(\"dating-binned.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "split dataset\n",
    "'''\n",
    "testset=dating.sample(frac=0.2,random_state=47)\n",
    "trainset=dating.drop(testset.index)\n",
    "testset.to_csv(\"testSet.csv\", index = False)\n",
    "trainset.to_csv(\"trainingSet.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for column gender bin count for decision 1 are {0: 969, 1: 1366}\n",
      "for column gender bin count for decision 0 are {0: 1676, 1: 1384}\n",
      "for column age bin count for decision 1 are {0: 1288, 2: 40, 4: 2, 1: 1005}\n",
      "for column age bin count for decision 0 are {0: 1701, 2: 37, 4: 1, 1: 1321}\n",
      "for column age_o bin count for decision 1 are {0: 1339, 2: 35, 4: 1, 1: 960}\n",
      "for column age_o bin count for decision 0 are {0: 1650, 2: 75, 4: 4, 1: 1331}\n",
      "for column race bin count for decision 1 are {0: 615, 2: 1242, 4: 179, 1: 143, 3: 156}\n",
      "for column race bin count for decision 0 are {0: 698, 2: 1822, 4: 176, 1: 126, 3: 238}\n",
      "for column race_o bin count for decision 1 are {0: 464, 2: 1401, 4: 148, 1: 120, 3: 202}\n",
      "for column race_o bin count for decision 0 are {0: 846, 2: 1625, 4: 208, 1: 158, 3: 223}\n",
      "for column samerace bin count for decision 1 are {0: 1353, 1: 982}\n",
      "for column samerace bin count for decision 0 are {0: 1862, 1: 1198}\n",
      "for column importance_same_race bin count for decision 1 are {0: 1123, 2: 332, 4: 155, 1: 455, 3: 270}\n",
      "for column importance_same_race bin count for decision 0 are {0: 1265, 2: 451, 4: 293, 1: 514, 3: 537}\n",
      "for column importance_same_religion bin count for decision 1 are {0: 1210, 2: 342, 4: 114, 1: 403, 3: 266}\n",
      "for column importance_same_religion bin count for decision 0 are {0: 1342, 2: 554, 4: 290, 1: 536, 3: 338}\n",
      "for column field bin count for decision 1 are {0: 1, 2: 2, 4: 2, 6: 1, 8: 2, 10: 5, 12: 5, 14: 31, 20: 6, 22: 9, 24: 2, 26: 25, 28: 3, 30: 3, 32: 2, 34: 3, 36: 31, 38: 2, 40: 9, 42: 6, 44: 1, 46: 14, 48: 4, 50: 1, 52: 5, 54: 7, 56: 6, 58: 9, 60: 2, 62: 1, 64: 4, 66: 3, 68: 4, 70: 72, 74: 46, 76: 1, 78: 5, 80: 47, 82: 38, 84: 3, 86: 1, 88: 5, 90: 15, 92: 3, 94: 4, 96: 10, 98: 5, 100: 4, 102: 2, 104: 2, 106: 2, 108: 3, 110: 3, 112: 3, 114: 1, 116: 4, 118: 8, 120: 15, 122: 3, 124: 7, 126: 1, 130: 2, 132: 17, 134: 4, 136: 8, 138: 4, 140: 23, 142: 150, 144: 7, 146: 15, 148: 31, 150: 1, 152: 4, 154: 3, 158: 9, 160: 20, 162: 4, 164: 4, 166: 3, 168: 19, 170: 9, 172: 1, 174: 34, 176: 2, 178: 53, 180: 12, 182: 1, 184: 14, 186: 20, 190: 2, 192: 83, 194: 49, 196: 4, 198: 1, 200: 1, 202: 5, 204: 11, 206: 1, 1: 11, 3: 4, 5: 6, 7: 4, 9: 8, 11: 7, 13: 7, 15: 5, 17: 42, 19: 21, 21: 5, 23: 156, 25: 4, 27: 7, 29: 3, 31: 3, 33: 2, 35: 8, 37: 12, 39: 3, 41: 33, 43: 6, 45: 1, 47: 11, 49: 8, 51: 2, 53: 4, 55: 1, 59: 7, 61: 1, 63: 51, 65: 3, 67: 4, 69: 14, 71: 9, 73: 6, 75: 16, 77: 1, 79: 10, 81: 8, 83: 11, 85: 5, 87: 3, 89: 5, 93: 6, 95: 8, 97: 2, 99: 4, 101: 6, 103: 3, 105: 82, 107: 4, 109: 7, 111: 9, 113: 7, 115: 14, 117: 9, 119: 1, 121: 186, 125: 9, 127: 6, 129: 2, 131: 9, 133: 7, 135: 2, 137: 6, 139: 7, 141: 10, 143: 2, 145: 8, 147: 8, 149: 19, 151: 11, 153: 2, 155: 9, 157: 10, 159: 2, 161: 10, 163: 2, 165: 7, 169: 2, 171: 4, 173: 2, 175: 15, 177: 28, 179: 5, 181: 7, 185: 4, 187: 2, 189: 6, 191: 1, 193: 6, 195: 2, 197: 2, 199: 7, 201: 7, 203: 5, 205: 11, 207: 19, 209: 11}\n",
      "for column field bin count for decision 0 are {0: 10, 2: 4, 4: 5, 6: 11, 8: 7, 10: 12, 12: 3, 14: 20, 16: 7, 18: 6, 20: 9, 22: 9, 24: 3, 26: 27, 28: 11, 30: 9, 32: 7, 34: 11, 36: 38, 38: 11, 40: 7, 42: 5, 44: 2, 46: 5, 48: 5, 50: 22, 52: 11, 54: 7, 56: 1, 58: 30, 60: 3, 62: 1, 64: 9, 66: 19, 68: 9, 70: 73, 72: 11, 74: 19, 76: 7, 78: 5, 80: 40, 82: 54, 84: 7, 86: 1, 90: 30, 92: 7, 96: 4, 98: 3, 100: 13, 104: 11, 106: 11, 108: 15, 110: 3, 114: 7, 116: 5, 118: 4, 120: 25, 122: 14, 124: 1, 126: 5, 128: 2, 130: 10, 134: 8, 136: 17, 140: 36, 142: 195, 144: 9, 146: 15, 148: 35, 150: 3, 152: 12, 154: 10, 160: 8, 162: 8, 164: 11, 166: 6, 168: 21, 170: 6, 172: 1, 174: 22, 178: 65, 180: 13, 182: 5, 184: 3, 186: 10, 188: 5, 190: 1, 192: 174, 194: 55, 196: 3, 198: 10, 200: 3, 202: 6, 204: 7, 206: 8, 208: 8, 3: 4, 5: 6, 7: 9, 9: 29, 11: 2, 13: 10, 15: 11, 17: 53, 19: 39, 21: 12, 23: 253, 25: 2, 27: 6, 29: 9, 31: 4, 33: 12, 35: 6, 37: 4, 39: 6, 41: 26, 43: 4, 47: 13, 49: 4, 51: 14, 53: 10, 55: 4, 57: 2, 61: 5, 63: 29, 65: 10, 67: 8, 69: 3, 71: 16, 73: 9, 75: 22, 77: 5, 79: 19, 81: 6, 83: 2, 85: 3, 87: 5, 89: 7, 91: 1, 93: 6, 95: 16, 97: 5, 99: 9, 101: 7, 103: 3, 105: 142, 107: 5, 111: 17, 113: 8, 115: 2, 117: 4, 121: 182, 123: 8, 125: 25, 127: 4, 129: 2, 131: 6, 133: 9, 137: 6, 139: 4, 141: 4, 143: 12, 145: 6, 147: 11, 151: 3, 153: 8, 155: 30, 157: 6, 159: 10, 161: 7, 163: 8, 165: 19, 167: 15, 169: 9, 171: 11, 173: 14, 177: 41, 179: 1, 181: 13, 183: 2, 185: 4, 187: 6, 189: 12, 191: 11, 193: 10, 195: 5, 197: 9, 199: 19, 201: 7, 203: 5, 205: 6, 207: 15, 209: 5}\n",
      "for column pref_o_attractive bin count for decision 1 are {0: 1513, 2: 132, 4: 15, 1: 649, 3: 26}\n",
      "for column pref_o_attractive bin count for decision 0 are {0: 1955, 2: 136, 4: 6, 1: 947, 3: 16}\n",
      "for column pref_o_sincere bin count for decision 1 are {0: 1955, 2: 3, 1: 377}\n",
      "for column pref_o_sincere bin count for decision 0 are {0: 2441, 2: 8, 1: 611}\n",
      "for column pref_o_intelligence bin count for decision 1 are {0: 1627, 2: 30, 1: 678}\n",
      "for column pref_o_intelligence bin count for decision 0 are {0: 2055, 2: 32, 1: 973}\n",
      "for column pref_o_funny bin count for decision 1 are {0: 1948, 2: 8, 1: 379}\n",
      "for column pref_o_funny bin count for decision 0 are {0: 2528, 2: 11, 1: 521}\n",
      "for column pref_o_ambitious bin count for decision 1 are {0: 2307, 1: 28}\n",
      "for column pref_o_ambitious bin count for decision 0 are {0: 3018, 1: 42}\n",
      "for column pref_o_shared_interests bin count for decision 1 are {0: 2276, 1: 59}\n",
      "for column pref_o_shared_interests bin count for decision 0 are {0: 2881, 1: 179}\n",
      "for column attractive_important bin count for decision 1 are {0: 1534, 2: 102, 4: 3, 1: 677, 3: 19}\n",
      "for column attractive_important bin count for decision 0 are {0: 1936, 2: 163, 4: 11, 1: 922, 3: 28}\n",
      "for column sincere_important bin count for decision 1 are {0: 1855, 2: 6, 1: 474}\n",
      "for column sincere_important bin count for decision 0 are {0: 2547, 2: 6, 1: 507}\n",
      "for column intelligence_important bin count for decision 1 are {0: 1523, 2: 12, 1: 800}\n",
      "for column intelligence_important bin count for decision 0 are {0: 2168, 2: 41, 1: 851}\n",
      "for column funny_important bin count for decision 1 are {0: 1871, 2: 10, 1: 454}\n",
      "for column funny_important bin count for decision 0 are {0: 2587, 2: 10, 1: 463}\n",
      "for column ambition_important bin count for decision 1 are {0: 2292, 1: 43}\n",
      "for column ambition_important bin count for decision 0 are {0: 3021, 1: 39}\n",
      "for column shared_interests_important bin count for decision 1 are {0: 2213, 1: 122}\n",
      "for column shared_interests_important bin count for decision 0 are {0: 2983, 1: 77}\n",
      "for column attractive bin count for decision 1 are {0: 12, 2: 563, 4: 291, 1: 120, 3: 1349}\n",
      "for column attractive bin count for decision 0 are {0: 3, 2: 621, 4: 407, 1: 103, 3: 1926}\n",
      "for column sincere bin count for decision 1 are {0: 10, 2: 185, 4: 1165, 1: 57, 3: 918}\n",
      "for column sincere bin count for decision 0 are {0: 19, 2: 209, 4: 1544, 1: 39, 3: 1249}\n",
      "for column intelligence bin count for decision 1 are {0: 14, 2: 352, 4: 784, 1: 79, 3: 1106}\n",
      "for column intelligence bin count for decision 0 are {0: 13, 2: 470, 4: 1068, 1: 71, 3: 1438}\n",
      "for column funny bin count for decision 1 are {2: 75, 4: 1195, 1: 10, 3: 1055}\n",
      "for column funny bin count for decision 0 are {2: 110, 4: 1478, 1: 8, 3: 1464}\n",
      "for column ambition bin count for decision 1 are {0: 24, 2: 393, 4: 818, 1: 117, 3: 983}\n",
      "for column ambition bin count for decision 0 are {0: 36, 2: 461, 4: 1094, 1: 141, 3: 1328}\n",
      "for column attractive_partner bin count for decision 1 are {0: 15, 2: 587, 4: 453, 1: 69, 3: 1211}\n",
      "for column attractive_partner bin count for decision 0 are {0: 221, 2: 1359, 4: 98, 1: 688, 3: 694}\n",
      "for column sincere_partner bin count for decision 1 are {0: 10, 2: 422, 4: 606, 1: 47, 3: 1250}\n",
      "for column sincere_partner bin count for decision 0 are {0: 63, 2: 891, 4: 510, 1: 224, 3: 1372}\n",
      "for column intelligence_parter bin count for decision 1 are {2: 353, 4: 649, 1: 20, 3: 1313}\n",
      "for column intelligence_parter bin count for decision 0 are {0: 29, 2: 846, 4: 549, 1: 131, 3: 1505}\n",
      "for column funny_partner bin count for decision 1 are {0: 11, 2: 582, 4: 476, 1: 60, 3: 1206}\n",
      "for column funny_partner bin count for decision 0 are {0: 210, 2: 1268, 4: 182, 1: 535, 3: 865}\n",
      "for column ambition_partner bin count for decision 1 are {0: 10, 2: 667, 4: 469, 1: 78, 3: 1111}\n",
      "for column ambition_partner bin count for decision 0 are {0: 87, 2: 1130, 4: 408, 1: 285, 3: 1150}\n",
      "for column shared_interests_partner bin count for decision 1 are {0: 60, 2: 880, 4: 285, 1: 191, 3: 919}\n",
      "for column shared_interests_partner bin count for decision 0 are {0: 501, 2: 1165, 4: 93, 1: 798, 3: 503}\n",
      "for column sports bin count for decision 1 are {0: 269, 2: 479, 4: 571, 1: 339, 3: 677}\n",
      "for column sports bin count for decision 0 are {0: 253, 2: 625, 4: 769, 1: 430, 3: 983}\n",
      "for column tvsports bin count for decision 1 are {0: 767, 2: 442, 4: 225, 1: 436, 3: 465}\n",
      "for column tvsports bin count for decision 0 are {0: 963, 2: 555, 4: 307, 1: 592, 3: 643}\n",
      "for column exercise bin count for decision 1 are {0: 285, 2: 607, 4: 375, 1: 357, 3: 711}\n",
      "for column exercise bin count for decision 0 are {0: 236, 2: 770, 4: 635, 1: 420, 3: 999}\n",
      "for column dining bin count for decision 1 are {0: 24, 2: 409, 4: 881, 1: 71, 3: 950}\n",
      "for column dining bin count for decision 0 are {0: 9, 2: 488, 4: 1204, 1: 70, 3: 1289}\n",
      "for column museums bin count for decision 1 are {0: 46, 2: 485, 4: 613, 1: 265, 3: 926}\n",
      "for column museums bin count for decision 0 are {0: 49, 2: 643, 4: 785, 1: 331, 3: 1252}\n",
      "for column art bin count for decision 1 are {0: 74, 2: 521, 4: 554, 1: 323, 3: 863}\n",
      "for column art bin count for decision 0 are {0: 106, 2: 724, 4: 669, 1: 430, 3: 1131}\n",
      "for column hiking bin count for decision 1 are {0: 324, 2: 567, 4: 341, 1: 463, 3: 640}\n",
      "for column hiking bin count for decision 0 are {0: 434, 2: 693, 4: 427, 1: 654, 3: 852}\n",
      "for column gaming bin count for decision 1 are {0: 785, 2: 516, 4: 87, 1: 551, 3: 396}\n",
      "for column gaming bin count for decision 0 are {0: 1288, 2: 620, 4: 105, 1: 670, 3: 377}\n",
      "for column clubbing bin count for decision 1 are {0: 292, 2: 627, 4: 320, 1: 344, 3: 752}\n",
      "for column clubbing bin count for decision 0 are {0: 436, 2: 697, 4: 404, 1: 539, 3: 984}\n",
      "for column reading bin count for decision 1 are {0: 37, 2: 378, 4: 967, 1: 112, 3: 841}\n",
      "for column reading bin count for decision 0 are {0: 66, 2: 447, 4: 1301, 1: 207, 3: 1039}\n",
      "for column tv bin count for decision 1 are {0: 419, 2: 700, 4: 212, 1: 405, 3: 599}\n",
      "for column tv bin count for decision 0 are {0: 539, 2: 899, 4: 354, 1: 565, 3: 703}\n",
      "for column theater bin count for decision 1 are {0: 89, 2: 608, 4: 595, 1: 261, 3: 782}\n",
      "for column theater bin count for decision 0 are {0: 145, 2: 688, 4: 822, 1: 374, 3: 1031}\n",
      "for column movies bin count for decision 1 are {0: 9, 2: 302, 4: 903, 1: 89, 3: 1032}\n",
      "for column movies bin count for decision 0 are {0: 31, 2: 388, 4: 1355, 1: 107, 3: 1179}\n",
      "for column concerts bin count for decision 1 are {0: 79, 2: 577, 4: 657, 1: 279, 3: 743}\n",
      "for column concerts bin count for decision 0 are {0: 105, 2: 813, 4: 720, 1: 346, 3: 1076}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for column music bin count for decision 1 are {0: 27, 2: 356, 4: 1000, 1: 75, 3: 877}\n",
      "for column music bin count for decision 0 are {0: 22, 2: 519, 4: 1243, 1: 84, 3: 1192}\n",
      "for column shopping bin count for decision 1 are {0: 407, 2: 639, 4: 372, 1: 360, 3: 557}\n",
      "for column shopping bin count for decision 0 are {0: 492, 2: 710, 4: 607, 1: 484, 3: 767}\n",
      "for column yoga bin count for decision 1 are {0: 765, 2: 503, 4: 245, 1: 441, 3: 381}\n",
      "for column yoga bin count for decision 0 are {0: 1058, 2: 595, 4: 271, 1: 657, 3: 479}\n",
      "for column interests_correlate bin count for decision 1 are {0: 2, 2: 862, 4: 218, 1: 252, 3: 1001}\n",
      "for column interests_correlate bin count for decision 0 are {0: 12, 2: 1146, 4: 253, 1: 365, 3: 1284}\n",
      "for column expected_happy_with_sd_people bin count for decision 1 are {0: 93, 2: 1133, 4: 116, 1: 390, 3: 603}\n",
      "for column expected_happy_with_sd_people bin count for decision 0 are {0: 172, 2: 1463, 4: 98, 1: 641, 3: 686}\n",
      "for column like bin count for decision 1 are {0: 5, 2: 589, 4: 337, 1: 37, 3: 1367}\n",
      "for column like bin count for decision 0 are {0: 215, 2: 1450, 4: 64, 1: 654, 3: 677}\n",
      "for column decision bin count for decision 1 are {1: 2335}\n",
      "for column decision bin count for decision 0 are {0: 3060}\n"
     ]
    }
   ],
   "source": [
    "trainset = pd.read_csv('trainingSet.csv')\n",
    "testset = pd.read_csv('testSet.csv')\n",
    "count_dict_yes = {}\n",
    "count_dict_no = {}\n",
    "for column in trainset:\n",
    "    count_dict_yes[column] = trainset[trainset['decision'] == 1][column].value_counts(sort=False).to_dict()\n",
    "    count_dict_no[column] = trainset[trainset['decision'] == 0][column].value_counts(sort=False).to_dict()\n",
    "    print ('for column', column, 'bin count for decision 1 are', \n",
    "           count_dict_yes[column])\n",
    "    print ('for column', column, 'bin count for decision 0 are',\n",
    "           count_dict_no[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_probability(attribute, value, decision):\n",
    "    if decision == 1:\n",
    "        value_count = 0\n",
    "        if value in count_dict_yes[attribute]:\n",
    "            value_count = count_dict_yes[attribute][value]\n",
    "        return (value_count+1.0)/(count_dict_yes['decision'][1]+len(count_dict_yes[attribute]))\n",
    "    elif decision == 0:\n",
    "        value_count = 0\n",
    "        if value in count_dict_no[attribute]:\n",
    "            value_count = count_dict_no[attribute][value]\n",
    "        return (value_count+1.0)/(count_dict_no['decision'][0]+len(count_dict_no[attribute]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prior_probability_dec_1 = count_dict_yes['decision'][1]/(count_dict_no['decision'][0]+count_dict_yes['decision'][1])\n",
    "prior_probability_dec_0 = count_dict_no['decision'][0]/(count_dict_no['decision'][0]+count_dict_yes['decision'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference_row(row):\n",
    "    correct = 0\n",
    "    prob_dec_1 = 1\n",
    "    prob_dec_0 = 1\n",
    "    for column in count_dict_yes:\n",
    "        if column != 'decision':\n",
    "#                 prob_dec_1 += np.log(get_probability(column, row[column], 1))\n",
    "#                 prob_dec_0 += np.log(get_probability(column, row[column], 0))\n",
    "                '''\n",
    "                dont take logs\n",
    "                '''\n",
    "                prob_dec_1 *= get_probability(column, row[column], 1)\n",
    "                prob_dec_0 *= get_probability(column, row[column], 0)\n",
    "    '''\n",
    "    multiply by prior probabilities\n",
    "    '''\n",
    "#     prob_dec_1 += np.log(prior_probability_dec_1)\n",
    "#     prob_dec_0 += np.log(prior_probability_dec_0)\n",
    "    # dont take logs\n",
    "    prob_dec_1 *= prior_probability_dec_1\n",
    "    prob_dec_0 *= prior_probability_dec_0\n",
    "    \n",
    "    predicted_value = 0\n",
    "    true_value = row['decision']\n",
    "    if prob_dec_1 > prob_dec_0:\n",
    "        predicted_value = 1\n",
    "    if predicted_value == true_value:\n",
    "        correct = 1\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(dataset):  \n",
    "    dataset['correct_prediction'] = dataset.apply(inference_row, axis=1)\n",
    "    correct_prediction_dict = dataset['correct_prediction'].value_counts().to_dict()\n",
    "    return correct_prediction_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_predictions = inference(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.77516218721038\n"
     ]
    }
   ],
   "source": [
    "print (correct_predictions[1]/(correct_predictions[1] + correct_predictions[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (correct_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in count_dict_yes:\n",
    "    print (column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in trainset:\n",
    "    print (column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
